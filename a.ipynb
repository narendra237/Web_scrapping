{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85286d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "334af86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Create a Service object with the path to chromedriver\n",
    "service = Service(executable_path=r'chromedriver-win64\\chromedriver.exe')\n",
    "\n",
    "# Initialize the Chrome driver with the service object\n",
    "driver = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ae79271",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.barcouncilofindia.org/info/22nd-quali-r6uwmu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abea82e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe40ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 links on the page\n",
      "Attempting to download: CLE Applications - Frequently Asked Questions (FAQs) (https://www.barcouncilofindia.org/info/cle-faq-01)\n",
      "Attempting to download: CLE Applications - Process Overview Diagram (https://www.barcouncilofindia.org/info/cle-flow-01)\n",
      "Attempting to download: CLE Application - CLE Seeking Approval of Fresh Course (https://www.barcouncilofindia.org/info/apply-for-cleNewCourse)\n",
      "Attempting to download: Central Information Commission (https://www.barcouncilofindia.org/l/cia-website)\n",
      "Attempting to download: Right to Information (https://www.barcouncilofindia.org/info/rti)\n",
      "Scraping completed:\n",
      "- 46 unique links saved to 'scraped_links.txt'\n",
      "- 0 files downloaded to 'downloads'\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import requests\n",
    "# from urllib.parse import urljoin, urlparse\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# def scrape_and_download_files(driver, url, output_file=\"scraped_links.txt\", download_dir=\"downloads\"):\n",
    "#     \"\"\"\n",
    "#     Enhanced scraper specifically for Bar Council of India website\n",
    "#     to better identify and download documents\n",
    "#     \"\"\"\n",
    "#     # Navigate to the URL\n",
    "#     driver.get(url)\n",
    "    \n",
    "#     # Wait for the page to load completely\n",
    "#     time.sleep(5)\n",
    "    \n",
    "#     # Create download directory if it doesn't exist\n",
    "#     if not os.path.exists(download_dir):\n",
    "#         os.makedirs(download_dir)\n",
    "    \n",
    "#     # Find all links on the page\n",
    "#     links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "#     print(f\"Found {len(links)} links on the page\")\n",
    "    \n",
    "#     # Create a file to store the links\n",
    "#     with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "#         file.write(f\"Base URL: {url}\\n\\n\")\n",
    "        \n",
    "#         # Track unique links and downloads\n",
    "#         unique_links = set()\n",
    "#         download_count = 0\n",
    "        \n",
    "#         # Keywords that might indicate a downloadable document\n",
    "#         download_indicators = ['download', 'pdf', 'doc', 'form', 'application', \n",
    "#                               'notification', 'result', 'bulletin', 'circular']\n",
    "        \n",
    "#         for i, link in enumerate(links, 1):\n",
    "#             try:\n",
    "#                 href = link.get_attribute('href')\n",
    "#                 if not href:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Skip duplicates\n",
    "#                 if href in unique_links:\n",
    "#                     continue\n",
    "                \n",
    "#                 unique_links.add(href)\n",
    "#                 text = link.text.strip() or \"[No text]\"\n",
    "#                 file.write(f\"{i}. Text: {text}\\nURL: {href}\\n\")\n",
    "                \n",
    "#                 # Check if this looks like a document link\n",
    "#                 is_likely_document = False\n",
    "                \n",
    "#                 # Method 1: Check URL patterns\n",
    "#                 file_url = urljoin(url, href)\n",
    "#                 parsed_url = urlparse(file_url)\n",
    "#                 filename = os.path.basename(parsed_url.path)\n",
    "#                 extension = os.path.splitext(filename)[1].lower()\n",
    "                \n",
    "#                 if extension in ['.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', '.rar']:\n",
    "#                     is_likely_document = True\n",
    "                \n",
    "#                 # Method 2: Check link text for download indicators\n",
    "#                 link_text_lower = text.lower()\n",
    "#                 if any(indicator in link_text_lower for indicator in download_indicators):\n",
    "#                     is_likely_document = True\n",
    "                    \n",
    "#                 # Method 3: Check if URL contains document indicators\n",
    "#                 if any(indicator in href.lower() for indicator in download_indicators):\n",
    "#                     is_likely_document = True\n",
    "                \n",
    "#                 if is_likely_document:\n",
    "#                     try:\n",
    "#                         print(f\"Attempting to download: {text} ({href})\")\n",
    "                        \n",
    "#                         # Generate a filename if needed\n",
    "#                         if not filename or filename == \"\" or \".\" not in filename:\n",
    "#                             # Create a filename from link text or use a default\n",
    "#                             clean_text = \"\".join(c for c in text if c.isalnum() or c in \" -_\")[:50]\n",
    "#                             clean_text = clean_text.strip().replace(\" \", \"_\") or \"document\"\n",
    "#                             filename = f\"{clean_text}.pdf\"  # Default to PDF\n",
    "                        \n",
    "#                         download_path = os.path.join(download_dir, filename)\n",
    "                        \n",
    "#                         # Download using requests\n",
    "#                         response = requests.get(file_url, stream=True, \n",
    "#                                                headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                        \n",
    "#                         if response.status_code == 200:\n",
    "#                             # Check if it's actually a document (Content-Type)\n",
    "#                             content_type = response.headers.get('Content-Type', '')\n",
    "#                             is_document = (\n",
    "#                                 'pdf' in content_type or \n",
    "#                                 'msword' in content_type or\n",
    "#                                 'application/' in content_type or\n",
    "#                                 'octet-stream' in content_type\n",
    "#                             )\n",
    "                            \n",
    "#                             if is_document or len(response.content) > 10000:  # Size check\n",
    "#                                 with open(download_path, 'wb') as f:\n",
    "#                                     for chunk in response.iter_content(1024):\n",
    "#                                         f.write(chunk)\n",
    "#                                 file.write(f\"   [✓] Downloaded to: {download_path}\\n\")\n",
    "#                                 download_count += 1\n",
    "#                                 print(f\"Successfully downloaded: {filename}\")\n",
    "#                             else:\n",
    "#                                 file.write(f\"   [✗] Not a document: {content_type}\\n\")\n",
    "#                         else:\n",
    "#                             file.write(f\"   [✗] Download failed: Status {response.status_code}\\n\")\n",
    "#                     except Exception as e:\n",
    "#                         file.write(f\"   [✗] Download error: {str(e)}\\n\")\n",
    "#                         print(f\"Error downloading {href}: {str(e)}\")\n",
    "#                 file.write(\"\\n\")\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 file.write(f\"{i}. Error extracting link: {str(e)}\\n\\n\")\n",
    "    \n",
    "#     print(f\"Scraping completed:\")\n",
    "#     print(f\"- {len(unique_links)} unique links saved to '{output_file}'\")\n",
    "#     print(f\"- {download_count} files downloaded to '{download_dir}'\")\n",
    "\n",
    "# # Run this function in a new cell\n",
    "# url_to_scrape = \"https://www.barcouncilofindia.org/info/22nd-quali-r6uwmu\"\n",
    "# scrape_and_download_files(\n",
    "#     driver=driver,\n",
    "#     url=url_to_scrape,\n",
    "#     output_file=\"scraped_links.txt\",\n",
    "#     download_dir=\"downloads\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "778c262a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0 clean document links to clean_links.txt\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def get_clean_links_from_page(driver, url, output_file=\"clean_links.txt\"):\n",
    "    driver.get(url)\n",
    "    import time\n",
    "    time.sleep(5)\n",
    "    links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "    unique_links = set()\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        for link in links:\n",
    "            href = link.get_attribute('href')\n",
    "            if not href:\n",
    "                continue\n",
    "            # Only process links that look like documents\n",
    "            if any(ext in href.lower() for ext in [\".pdf\", \".doc\", \".docx\", \".xls\", \".xlsx\"]):\n",
    "                # Remove query parameters for a clean link\n",
    "                parsed = urlparse(href)\n",
    "                clean_link = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "                if clean_link not in unique_links:\n",
    "                    unique_links.add(clean_link)\n",
    "                    file.write(clean_link + \"\\n\")\n",
    "    print(f\"Saved {len(unique_links)} clean document links to {output_file}\")\n",
    "\n",
    "# Usage:\n",
    "get_clean_links_from_page(\n",
    "    driver=driver,\n",
    "    url=\"https://www.barcouncilofindia.org/info/22nd-quali-r6uwmu\",\n",
    "    output_file=\"clean_links.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00e03a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open all links from the .txt file in the browser using the existing driver\n",
    "\n",
    "# links_file = \"scraped_links.txt\"\n",
    "\n",
    "# with open(links_file, \"r\", encoding=\"utf-8\") as file:\n",
    "#     lines = file.readlines()\n",
    "\n",
    "# # Extract URLs from the file (lines starting with \"URL:\")\n",
    "# urls = [line.strip().split(\"URL:\")[1].strip() for line in lines if line.strip().startswith(\"URL:\")]\n",
    "\n",
    "# for url in urls:\n",
    "#     driver.execute_script(f\"window.open('{url}', '_blank');\")\n",
    "#     time.sleep(0.5)  # Optional: slight delay to avoid overloading the browser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
